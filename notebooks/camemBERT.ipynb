{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09299167-9a7d-48af-a38e-49b6cf76f975",
   "metadata": {},
   "source": [
    "This jupyter notebook is a direct copy and adaptation of the code presented [here](https://ledatascientist.com/analyse-de-sentiments-avec-camembert/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ca699-9e08-4815-8f11-9949bc44cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b3979-9a97-4f7c-97a3-f110e65fdf1b",
   "metadata": {},
   "source": [
    "One also need the `SentencePiece library` for `CamembertTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c5868-f19c-4f63-85e1-ef9484aca8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentencepiece\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da7083-4fd4-4a8d-9f3f-1114d44a10ec",
   "metadata": {},
   "source": [
    "# Text encoding\n",
    "\n",
    "## Loading the dataset\n",
    "One need to encode the text from the dataset (here it will be aclIMDB) into a vectorial space, this is called embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b7939-55a1-49c5-a3d3-e9b1712c07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allocine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e410c91-7089-41f7-8780-b88069515f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57137e7-b94b-4f25-bded-778fb34739cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eebbc3-7f30-4e0a-8aab-032ce68344a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"]\n",
    "dataset_validation = dataset[\"validation\"]\n",
    "dataset_test = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c7790-d2d4-4f7c-9aac-0a2983206310",
   "metadata": {},
   "source": [
    "We can request specific attributes of the dataset, like `description`, `citation`, and `homepage`, by calling them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bd7e4-a033-46ca-bef0-8acd388dab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228abadd-f8ef-4fa7-8538-6ad6963cbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e848c68-6247-4050-a5bb-e545e309c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2042fe6-690e-4374-9948-47890d30a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49411b6e-1eac-4441-8c18-bc8e83c0ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53c50b-839c-4995-a93f-476a6d5382ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd71fe6-f50b-4d47-883b-e5177579875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee0e62-1376-49ef-9b7c-c9454db77da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6055f6-4468-4c31-b824-92a58ec9be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19148fce-3aa0-40ae-9054-4e6fe2b50a47",
   "metadata": {},
   "source": [
    "## Tokenizer / encoder\n",
    "* We will use the tokenizer of camemBERT to perform the embedding\n",
    "* We can change the pre-trained model (2nd parameter):\n",
    "\n",
    "|             Model                    | #params | Arch. |      Training data                |\n",
    "| :----------------------------------- | :-----  | :---  | :-------------------------------  |\n",
    "camembert-base                         | 110M    | Base  | OSCAR (138 GB of text)            |\n",
    "camembert/camembert-large              | 335M    | Large | CCNet (135 GB of text)            |\n",
    "camembert/camembert-base-ccnet         | 110M    | Base  | CCNet (135 GB of text)            |\n",
    "camembert/camembert-base-wikipedia-4gb | 110M    | Base  | Wikipedia (4 GB of text)          |\n",
    "camembert/camembert-base-oscar-4gb     | 110M    | Base  | Subsample of OSCAR (4 GB of text) |\n",
    "camembert/camembert-base-ccnet-4gb     | 110M    | Base  | Subsample of CCNet (4 GB of text) |\n",
    "\n",
    "* `do_lower_case=True` allow to lower all the characters (if there is Upper case characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9d4f3-c85f-44d1-a537-e8954359c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00fab8-5120-41ae-bd67-46824af64189",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = train_text + test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78690c8-2f46-4302-93ca-c1312977898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026faa8b-8769-401d-af7d-62b4a4dbfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction batch_encode_plus encode un batch de donnees\n",
    "# This cell can take a long time (~ 1 min for me)\n",
    "train_encoded_batch = Tokenizer.batch_encode_plus(train_text,\n",
    "                                                  add_special_tokens=True,\n",
    "                                                  max_length=MAX_LENGTH,\n",
    "                                                  padding=True,\n",
    "                                                  truncation=True,\n",
    "                                                  return_attention_mask = True,\n",
    "                                                  return_tensors = 'pt')\n",
    "\n",
    "test_encoded_batch = Tokenizer.batch_encode_plus(test_text,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 max_length=MAX_LENGTH,\n",
    "                                                 padding=True,\n",
    "                                                 truncation=True,\n",
    "                                                 return_attention_mask = True,\n",
    "                                                 return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27fba7-c741-4456-b773-aa061433a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform the sentiment list into a torch tensor\n",
    "train_sentiment = torch.tensor(train_sentiment)\n",
    "test_sentiment = torch.tensor(test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a4a0e-b0fe-49cc-a124-fc85a06697b3",
   "metadata": {},
   "source": [
    "The split of the dataset into a train and test sets are already performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0fb5b-fb91-4000-85d2-44c5a091c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encoded_batch['input_ids'],\n",
    "                              train_encoded_batch['attention_mask'],\n",
    "                              train_sentiment)\n",
    "test_dataset = TensorDataset(test_encoded_batch['input_ids'],\n",
    "                                   test_encoded_batch['attention_mask'],\n",
    "                                   test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce8f59-819c-4e92-b1cc-21979157554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dbe0f-ad65-430a-adb2-06948a039439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the DataLoaders object for train and test\n",
    "# A dataloader is an iterable object\n",
    "# Here, there are configure so that the batch are constructed randomly\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              sampler = RandomSampler(train_dataset),\n",
    "                              batch_size = batch_size)\n",
    " \n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             sampler = SequentialSampler(test_dataset),\n",
    "                             batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940b1d0-1445-4cca-a2cd-4007083c62f1",
   "metadata": {},
   "source": [
    "## Model loading:\n",
    "Thanks to the module transformers, we only need one line of code to retrieve the pre-trained Camembert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f9695-0d5e-47fc-b47e-417432e62f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading of the pre-trained model:\n",
    "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b195ab-d1aa-4d8e-b5eb-85d05f41597c",
   "metadata": {},
   "source": [
    "## Fine tuning / Hyperparametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec9f93-e56e-4ecd-86a5-b582a13bc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd81e3-f973-4f2e-ab54-6e2fcdaf9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torcch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b8df5-ae57-48ff-a2a0-daea980fc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour enregistrer les stats a chaque epoque\n",
    "training_stats = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eace815-10d4-480e-80e1-8a53732c2769",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2b974-6607-4eea-a272-c9cdd6f04a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entrainement\n",
    "for epoch in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch+1} / {epochs} ##########')\n",
    "    print('Training...')\n",
    " \n",
    "    # initialization of loss for the current epoch\n",
    "    total_train_loss = 0\n",
    " \n",
    "    # Calld of one round of 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "    model.train()\n",
    "\n",
    "    # Pour chaque batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # On fait un print chaque 40 batchs\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print(f'Batch {step} of {len(train_dataloader)}.')\n",
    "\n",
    "        # On recupere les donnees du batch\n",
    "        input_id = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        sentiment = batch[2].to(device)\n",
    "\n",
    "        # On met le gradient a 0\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        loss, logits = model(input_id,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=attention_mask,\n",
    "                             labels=sentiment)\n",
    "\n",
    "        # On incremente la loss totale\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backpropagtion\n",
    "        loss.backward()\n",
    "\n",
    "        # On actualise les parametrer grace a l'optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    # On calcule la  loss moyenne sur toute l'epoque\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    \n",
    "    # Enregistrement des stats de l'epoque\n",
    "    training_stats.append({'epoch': epoch + 1,'Training Loss': avg_train_loss})\n",
    "\n",
    "print(\"Model saved!\")\n",
    "torch.save(model.state_dict(), \"./sentiments.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60907149-cf52-4c2c-b144-fc5e97cfb588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8ab31-fa7e-442c-9da3-eea6495a67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_reviews, sentiments=None):\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_reviews,\n",
    "                                                truncation=True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors = 'pt')\n",
    "    if sentiments:\n",
    "        sentiments = torch.tensor(sentiments)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], sentiments\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']\n",
    " \n",
    "def predict(reviews, model=model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids, attention_mask = preprocess(reviews)\n",
    "        retour = model(input_ids, attention_mask=attention_mask)\n",
    "         \n",
    "        return torch.argmax(retour[0], dim=1)\n",
    " \n",
    " \n",
    "def evaluate(reviews, sentiments):\n",
    "    predictions = predict(reviews)\n",
    "    print(metrics.f1_score(sentiments, predictions, average='weighted', zero_division=0))\n",
    "    seaborn.heatmap(metrics.confusion_matrix(sentiments, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf2c40-4c37-40e2-8307-6be3e5ea0f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
