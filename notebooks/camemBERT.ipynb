{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ca699-9e08-4815-8f11-9949bc44cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b3979-9a97-4f7c-97a3-f110e65fdf1b",
   "metadata": {},
   "source": [
    "One also need the `SentencePiece library` for `CamembertTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c5868-f19c-4f63-85e1-ef9484aca8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da7083-4fd4-4a8d-9f3f-1114d44a10ec",
   "metadata": {},
   "source": [
    "# Text encoding\n",
    "\n",
    "## Loading the dataset\n",
    "One need to encode the text from the dataset (here it will be aclIMDB) into a vectorial space, this is called embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d85b68-feab-4d41-bf7b-aca537b364e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/processed/aclImdb/\"\n",
    "train_set_file = \"aclImdb_train.csv\"\n",
    "test_set_file = \"aclImdb_test.csv\"\n",
    "\n",
    "# Dataset loading:\n",
    "data_train = pd.read_csv(dataset_path + train_set_file)\n",
    "data_test = pd.read_csv(dataset_path + test_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eebbc3-7f30-4e0a-8aab-032ce68344a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data_train['text'].to_list()\n",
    "sentiment = data_train['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19148fce-3aa0-40ae-9054-4e6fe2b50a47",
   "metadata": {},
   "source": [
    "## Tokenizer / encoder\n",
    "* We will use the tokenizer of camemBERT to perform the embedding\n",
    "* We can change the pre-trained model (2nd parameter):\n",
    "\n",
    "|             Model                    | #params | Arch. |      Training data                |\n",
    "| :----------------------------------- | :-----  | :---  | :-------------------------------  |\n",
    "camembert-base                         | 110M    | Base  | OSCAR (138 GB of text)            |\n",
    "camembert/camembert-large              | 335M    | Large | CCNet (135 GB of text)            |\n",
    "camembert/camembert-base-ccnet         | 110M    | Base  | CCNet (135 GB of text)            |\n",
    "camembert/camembert-base-wikipedia-4gb | 110M    | Base  | Wikipedia (4 GB of text)          |\n",
    "camembert/camembert-base-oscar-4gb     | 110M    | Base  | Subsample of OSCAR (4 GB of text) |\n",
    "camembert/camembert-base-ccnet-4gb     | 110M    | Base  | Subsample of CCNet (4 GB of text) |\n",
    "\n",
    "* `do_lower_case=True` allow to lower all the characters (if there is Upper case characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78690c8-2f46-4302-93ca-c1312977898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75c3e1-71fe-460e-9eee-ef7935fe29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_text = list(map(len, text))\n",
    "MAX_LENGTH = max(l_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026faa8b-8769-401d-af7d-62b4a4dbfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction batch_encode_plus encode un batch de donnees\n",
    "# This cell can take a long time (~ 1 min for me)\n",
    "encoded_batch = Tokenizer.batch_encode_plus(text,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=MAX_LENGTH,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27fba7-c741-4456-b773-aa061433a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform the sentiment list into a torch tensor\n",
    "sentiment = torch.tensor(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240345b-ccbd-4ba2-a917-db7521906e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
